{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is going to be to combine the training and test sets so that any data transformations / feature engineering is easily applied to both. Only the training set is labeled, so I will create values of -999 for `Survived` in the test subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.concat(\n",
    "    [train.assign(Train = 1), \n",
    "    test.assign(Train = 0).assign(Survived = -999)[list(train) + ['Train']]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with `Name` - create last name, title, family features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_lastname = lambda x: x.split(',')[0]\n",
    "\n",
    "def extract_title(x):\n",
    "    title = x.split(',')[1].split('.')[0][1:]\n",
    "    if title in ['Mlle', 'Ms']:\n",
    "        title = 'Miss'\n",
    "    elif title == 'Mme':\n",
    "        title = 'Mrs'\n",
    "    elif title in ['Rev', 'Dr', 'Major', 'Col', 'Capt', 'Jonkheer', 'Dona']:\n",
    "        title = 'Esteemed'\n",
    "    elif title in ['Don', 'Lady', 'Sir', 'the Countess']:\n",
    "        title = 'Royalty'\n",
    "    return title\n",
    "    \n",
    "data = (data\n",
    "    .assign(LastName = lambda x: x.Name.map(extract_lastname))\n",
    "    .assign(Title = lambda x: x.Name.map(extract_title))\n",
    "    .assign(FamSize = lambda x: x.SibSp + x.Parch + 1)\n",
    "    .assign(Family = lambda x: [a + '_' + str(b) for a, b in \n",
    "                                zip(list(x.LastName), list(x.FamSize))])\n",
    "    .drop(['Name', 'SibSp', 'Parch', 'LastName'], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with `Ticket` - to reduce overfitting, create dummy variables for tickets that are shared among two or more passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ticket_counts(data):\n",
    "    ticket_to_count = dict(data.Ticket.value_counts())\n",
    "    data['TicketCount'] = data['Ticket'].map(ticket_to_count.get)\n",
    "    data['Ticket'] = np.where(data['TicketCount'] > 1, data['Ticket'], np.nan)\n",
    "    return data.drop(['TicketCount'], axis = 1)\n",
    "\n",
    "data = data.pipe(ticket_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the `Cabin` feature - creating a deck feature (the letter in the cabin name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_letter = np.vectorize(lambda x: x[:1]) \n",
    "\n",
    "data = (data\n",
    "        .assign(Deck = lambda x: np.where(\n",
    "            pd.notnull(x.Cabin), first_letter(x.Cabin.fillna('z')), x.Cabin))\n",
    "        .assign(Deck = lambda x: np.where(x.Deck == 'T', np.nan, x.Deck))\n",
    "        .drop(['Cabin'], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns we don't need, convert Sex to a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .drop(['PassengerId'], axis = 1)\n",
    "        .assign(Sex = lambda x: np.where(x.Sex == 'male', 1, 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy variables for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dummy_nans(data, col_name):\n",
    "    deck_cols = [col for col in list(data) if col_name in col]\n",
    "    for deck_col in deck_cols:\n",
    "        data[deck_col] = np.where(\n",
    "            data[col_name + 'nan'] == 1.0, np.nan, data[deck_col])\n",
    "    return data.drop([col_name + 'nan'], axis = 1)\n",
    "\n",
    "data = (data\n",
    "        .assign(Pclass = lambda x: x.Pclass.astype(str))\n",
    "        .pipe(pd.get_dummies, columns = ['Pclass', 'Family', 'Title', 'Ticket'])\n",
    "        .pipe(pd.get_dummies, columns = ['Deck'], dummy_na = True)\n",
    "        .pipe(pd.get_dummies, columns = ['Embarked'], dummy_na = True)\n",
    "        .pipe(create_dummy_nans, 'Deck_')\n",
    "        .pipe(create_dummy_nans, 'Embarked_')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise distances between 1309 samples\n",
      "Computing distances for sample #1/1309, elapsed time: 1.783\n",
      "Computing distances for sample #101/1309, elapsed time: 2.440\n",
      "Computing distances for sample #201/1309, elapsed time: 3.114\n",
      "Computing distances for sample #301/1309, elapsed time: 3.789\n",
      "Computing distances for sample #401/1309, elapsed time: 4.447\n",
      "Computing distances for sample #501/1309, elapsed time: 5.098\n",
      "Computing distances for sample #601/1309, elapsed time: 5.758\n",
      "Computing distances for sample #701/1309, elapsed time: 6.416\n",
      "Computing distances for sample #801/1309, elapsed time: 7.077\n",
      "Computing distances for sample #901/1309, elapsed time: 7.732\n",
      "Computing distances for sample #1001/1309, elapsed time: 8.386\n",
      "Computing distances for sample #1101/1309, elapsed time: 9.040\n",
      "Computing distances for sample #1201/1309, elapsed time: 9.700\n",
      "Computing distances for sample #1301/1309, elapsed time: 10.360\n",
      "Imputing row 1/1309 with 7 missing columns, elapsed time: 10.526\n",
      "Imputing row 101/1309 with 7 missing columns, elapsed time: 10.540\n",
      "Imputing row 201/1309 with 7 missing columns, elapsed time: 10.553\n",
      "Imputing row 301/1309 with 8 missing columns, elapsed time: 10.565\n",
      "Imputing row 401/1309 with 7 missing columns, elapsed time: 10.577\n",
      "Imputing row 501/1309 with 7 missing columns, elapsed time: 10.590\n",
      "Imputing row 601/1309 with 7 missing columns, elapsed time: 10.603\n",
      "Imputing row 701/1309 with 0 missing columns, elapsed time: 10.616\n",
      "Imputing row 801/1309 with 7 missing columns, elapsed time: 10.627\n",
      "Imputing row 901/1309 with 7 missing columns, elapsed time: 10.640\n",
      "Imputing row 1001/1309 with 0 missing columns, elapsed time: 10.652\n",
      "Imputing row 1101/1309 with 7 missing columns, elapsed time: 10.664\n",
      "Imputing row 1201/1309 with 7 missing columns, elapsed time: 10.676\n",
      "Imputing row 1301/1309 with 7 missing columns, elapsed time: 10.688\n",
      "[KNN] Warning: 4/1527603 still missing after imputation, replacing with 0\n",
      "Number of NAs: 0\n"
     ]
    }
   ],
   "source": [
    "def impute(data):\n",
    "    impute_missing = data.drop(['Survived', 'Train'], axis = 1)\n",
    "    impute_missing_cols = list(impute_missing)\n",
    "    filled_soft = fancyimpute.KNN().complete(np.array(impute_missing))\n",
    "    results = pd.DataFrame(filled_soft, columns = impute_missing_cols)\n",
    "    results['Train'] = list(data['Train'])\n",
    "    results['Survived'] = list(data['Survived'])\n",
    "    assert results.isnull().sum().sum() == 0, 'Not all NAs removed'\n",
    "    return results\n",
    "\n",
    "data = data.pipe(impute)\n",
    "print 'Number of NAs:', data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into separate training and predicting sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcomes = np.array(data.query('Train == 1')['Survived'])\n",
    "train = (data.query('Train == 1')\n",
    "         .drop(['Train', 'Survived'], axis = 1))\n",
    "to_predict = (data.query('Train == 0')\n",
    "              .drop(['Train', 'Survived'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, randomly split the training set into training and test sets using hold-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, outcomes, test_size = 0.2, random_state = 50)\n",
    "\n",
    "\n",
    "def train_test_model(model, hyperparameters, X_train, X_test, y_train, y_test,\n",
    "                    folds = 5):\n",
    "    \"\"\"\n",
    "    Given a [model] and a set of possible [hyperparameters], along with \n",
    "    matricies corresponding to hold-out cross-validation, returns a model w/ \n",
    "    optimized hyperparameters, and prints out model evaluation metrics.\n",
    "    \"\"\"\n",
    "    optimized_model = GridSearchCV(model, hyperparameters, cv = folds, n_jobs = -1)\n",
    "    optimized_model.fit(X_train, y_train)\n",
    "    predicted = optimized_model.predict(X_test)\n",
    "    print 'Optimized parameters:', optimized_model.best_params_\n",
    "    print 'Model accuracy (hold-out):', optimized_model.score(X_test, y_test)\n",
    "    kfold_score = np.mean(cross_val_score(\n",
    "            optimized_model.best_estimator_, np.append(X_train, X_test, axis = 0), \n",
    "            np.append(y_train, y_test), cv = folds))\n",
    "    print 'Model accuracy ({0}-fold):'.format(str(folds)), kfold_score, '\\n'\n",
    "    return optimized_model\n",
    "\n",
    "\n",
    "def create_submission(name, model, train, outcomes, to_predict):\n",
    "    \"\"\"\n",
    "    Train [model] on [train] and predict the probabilties on [test], and\n",
    "    format the submission according to Kaggle.\n",
    "    \"\"\"\n",
    "    model.fit(np.array(train), outcomes)\n",
    "    probs = model.predict(np.array(to_predict))\n",
    "    results = pd.DataFrame(probs, columns = ['Survived'])\n",
    "    results['PassengerId'] = list(pd.read_csv('data/test.csv')['PassengerId'])\n",
    "    (results[['PassengerId', 'Survived']]\n",
    "        .to_csv('submissions/' + name, index = False))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model: random forest - scores ~ .799 on the public leaderboard, ~ 0.841 in local 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'n_estimators': 500}\n",
      "Model accuracy (hold-out): 0.826815642458\n",
      "Model accuracy (5-fold): 0.841728045416 \n",
      "\n",
      "CPU times: user 12.2 s, sys: 72 ms, total: 12.3 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = train_test_model(\n",
    "    RandomForestClassifier(), {'n_estimators': [500, 800, 1000]}, \n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model: logistic regression - scores ~ .799 on the public leaderboard, 0.845 in local 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'C': 1}\n",
      "Model accuracy (hold-out): 0.843575418994\n",
      "Model accuracy (5-fold): 0.84510517993 \n",
      "\n",
      "CPU times: user 660 ms, sys: 84 ms, total: 744 ms\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = train_test_model(\n",
    "    LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model: SVM w/ Gaussian kernal - scores ~ .770 on the public leaderboard, ~ .802 in local 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'C': 100}\n",
      "Model accuracy (hold-out): 0.804469273743\n",
      "Model accuracy (5-fold): 0.802451922787 \n",
      "\n",
      "CPU times: user 4.47 s, sys: 132 ms, total: 4.6 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm_model = train_test_model(\n",
    "    SVC(), {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth model: Gradient Boosted Trees - scores ~.794 on the public leaderboard, .828 in local 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.139045+0.00652761\ttest-error:0.192135+0.0223028\n",
      "[1]\ttrain-error:0.13455+0.00630604\ttest-error:0.18764+0.0153239\n",
      "[2]\ttrain-error:0.130337+0.00337075\ttest-error:0.177528+0.0135765\n",
      "[3]\ttrain-error:0.126124+0.00257405\ttest-error:0.168539+0.0158901\n",
      "[4]\ttrain-error:0.127528+0.00578402\ttest-error:0.168539+0.0123084\n",
      "[5]\ttrain-error:0.124719+0.0027232\ttest-error:0.170786+0.0126124\n",
      "[6]\ttrain-error:0.124719+0.00520978\ttest-error:0.173033+0.0108937\n",
      "[7]\ttrain-error:0.126404+0.00502484\ttest-error:0.174157+0.00794505\n",
      "[8]\ttrain-error:0.124719+0.00473379\ttest-error:0.173033+0.0114585\n",
      "[9]\ttrain-error:0.12191+0.00513351\ttest-error:0.170786+0.0149062\n",
      "[10]\ttrain-error:0.124158+0.0036191\ttest-error:0.170786+0.0153239\n",
      "[11]\ttrain-error:0.122753+0.00302507\ttest-error:0.170786+0.0144766\n",
      "[12]\ttrain-error:0.120506+0.00550445\ttest-error:0.161797+0.0195906\n",
      "[13]\ttrain-error:0.119663+0.00473372\ttest-error:0.166292+0.019655\n",
      "[14]\ttrain-error:0.119663+0.00286477\ttest-error:0.166292+0.019655\n",
      "[15]\ttrain-error:0.120225+0.00522478\ttest-error:0.168539+0.0154878\n",
      "[16]\ttrain-error:0.11854+0.00383099\ttest-error:0.170786+0.0190018\n",
      "[17]\ttrain-error:0.119944+0.00522502\ttest-error:0.169663+0.0160482\n",
      "[18]\ttrain-error:0.117978+0.00809262\ttest-error:0.167415+0.0182563\n",
      "[19]\ttrain-error:0.117978+0.00682291\ttest-error:0.167415+0.0182563\n",
      "[20]\ttrain-error:0.117697+0.0063684\ttest-error:0.170786+0.0202871\n",
      "[21]\ttrain-error:0.117416+0.00631849\ttest-error:0.170786+0.0179776\n",
      "[22]\ttrain-error:0.117135+0.0071943\ttest-error:0.164045+0.0205345\n",
      "[23]\ttrain-error:0.116011+0.00593222\ttest-error:0.168539+0.0137612\n",
      "[24]\ttrain-error:0.117978+0.0046156\ttest-error:0.165168+0.0161267\n",
      "[25]\ttrain-error:0.11573+0.00499332\ttest-error:0.167415+0.0189352\n",
      "[26]\ttrain-error:0.116573+0.00502484\ttest-error:0.167415+0.0168165\n",
      "[27]\ttrain-error:0.116573+0.00670648\ttest-error:0.166292+0.0144766\n",
      "[28]\ttrain-error:0.11545+0.00571556\ttest-error:0.167415+0.0160482\n",
      "[29]\ttrain-error:0.11545+0.00661155\ttest-error:0.168539+0.0158901\n",
      "[30]\ttrain-error:0.115169+0.00510269\ttest-error:0.166292+0.0131033\n",
      "[31]\ttrain-error:0.116011+0.00619271\ttest-error:0.166292+0.0161267\n",
      "[32]\ttrain-error:0.11545+0.0053591\ttest-error:0.164045+0.0156499\n",
      "[33]\ttrain-error:0.115731+0.00679972\ttest-error:0.161797+0.0148213\n",
      "[34]\ttrain-error:0.116011+0.00537401\ttest-error:0.162921+0.0137612\n",
      "[35]\ttrain-error:0.117135+0.00491372\ttest-error:0.164045+0.0139435\n",
      "[36]\ttrain-error:0.116012+0.00431519\ttest-error:0.167415+0.0119968\n",
      "[37]\ttrain-error:0.116573+0.00478355\ttest-error:0.165168+0.0131033\n",
      "[38]\ttrain-error:0.116011+0.00422275\ttest-error:0.166292+0.0121015\n",
      "[39]\ttrain-error:0.116854+0.00429676\ttest-error:0.164045+0.0119968\n",
      "[40]\ttrain-error:0.115731+0.00315322\ttest-error:0.162921+0.0123084\n",
      "[41]\ttrain-error:0.115169+0.00344039\ttest-error:0.165168+0.0115682\n",
      "[42]\ttrain-error:0.113764+0.00435162\ttest-error:0.165168+0.0144766\n",
      "[43]\ttrain-error:0.114326+0.00383094\ttest-error:0.167415+0.0130066\n",
      "[44]\ttrain-error:0.112921+0.00372639\ttest-error:0.167415+0.0164369\n",
      "[45]\ttrain-error:0.11236+0.00426013\ttest-error:0.169663+0.0179072\n",
      "[46]\ttrain-error:0.113483+0.0044768\ttest-error:0.167415+0.0148213\n",
      "[47]\ttrain-error:0.112922+0.0044944\ttest-error:0.165168+0.0172611\n",
      "[48]\ttrain-error:0.112641+0.0048163\ttest-error:0.165168+0.0144766\n",
      "[49]\ttrain-error:0.112641+0.0048163\ttest-error:0.162921+0.0154878\n",
      "[50]\ttrain-error:0.11236+0.00387172\ttest-error:0.160674+0.017623\n",
      "[51]\ttrain-error:0.113202+0.00361925\ttest-error:0.165168+0.0144766\n",
      "[52]\ttrain-error:0.113203+0.00412849\ttest-error:0.167415+0.0139435\n",
      "[53]\ttrain-error:0.113203+0.0037267\ttest-error:0.165168+0.0144766\n",
      "[54]\ttrain-error:0.112922+0.00383099\ttest-error:0.164045+0.0156499\n",
      "[55]\ttrain-error:0.113202+0.00522506\ttest-error:0.164045+0.0143891\n",
      "[56]\ttrain-error:0.113483+0.00464953\ttest-error:0.164045+0.0143891\n",
      "[57]\ttrain-error:0.111798+0.00507181\ttest-error:0.161797+0.0148213\n",
      "[58]\ttrain-error:0.111236+0.00447682\ttest-error:0.161797+0.0139435\n",
      "[59]\ttrain-error:0.110955+0.0047003\ttest-error:0.160674+0.0161267\n",
      "[60]\ttrain-error:0.110394+0.00483289\ttest-error:0.165168+0.0135765\n",
      "[61]\ttrain-error:0.109551+0.00470042\ttest-error:0.161797+0.0130066\n",
      "[62]\ttrain-error:0.10927+0.00456408\ttest-error:0.161797+0.0156499\n",
      "[63]\ttrain-error:0.108427+0.00520992\ttest-error:0.162921+0.0174067\n",
      "[64]\ttrain-error:0.10927+0.00489762\ttest-error:0.162921+0.0170402\n",
      "[65]\ttrain-error:0.107865+0.00528514\ttest-error:0.165168+0.0161267\n",
      "[66]\ttrain-error:0.108146+0.00425999\ttest-error:0.164045+0.0143891\n",
      "[67]\ttrain-error:0.107585+0.00449439\ttest-error:0.164045+0.0130066\n",
      "[68]\ttrain-error:0.107585+0.00483266\ttest-error:0.165168+0.0190018\n",
      "[69]\ttrain-error:0.107304+0.00565985\ttest-error:0.165168+0.0190018\n",
      "[70]\ttrain-error:0.107304+0.00491388\ttest-error:0.167415+0.0164369\n",
      "[71]\ttrain-error:0.107585+0.00499355\ttest-error:0.167415+0.0179072\n",
      "[72]\ttrain-error:0.106742+0.00494558\ttest-error:0.166292+0.0179776\n",
      "[73]\ttrain-error:0.106742+0.00502489\ttest-error:0.169663+0.0164369\n",
      "[74]\ttrain-error:0.107303+0.00449451\ttest-error:0.169663+0.0164369\n",
      "[75]\ttrain-error:0.107304+0.00475053\ttest-error:0.169663+0.0164369\n",
      "[76]\ttrain-error:0.107304+0.00491388\ttest-error:0.168539+0.01465\n",
      "[77]\ttrain-error:0.106742+0.00510269\ttest-error:0.169663+0.0164369\n",
      "[78]\ttrain-error:0.106461+0.00543246\ttest-error:0.170786+0.0168914\n",
      "[79]\ttrain-error:0.10618+0.0055899\ttest-error:0.168539+0.01465\n",
      "[80]\ttrain-error:0.10618+0.0050717\ttest-error:0.167415+0.0148213\n",
      "[81]\ttrain-error:0.105337+0.00502495\ttest-error:0.166292+0.0144766\n",
      "[82]\ttrain-error:0.104775+0.0053739\ttest-error:0.169663+0.0143891\n",
      "[83]\ttrain-error:0.104775+0.00551883\ttest-error:0.167415+0.0160482\n",
      "[84]\ttrain-error:0.105337+0.0047003\ttest-error:0.166292+0.0161267\n",
      "[85]\ttrain-error:0.104775+0.00522517\ttest-error:0.166292+0.0161267\n",
      "[86]\ttrain-error:0.103933+0.00426026\ttest-error:0.167415+0.0192657\n",
      "[87]\ttrain-error:0.10309+0.00403185\ttest-error:0.168539+0.0200996\n",
      "[88]\ttrain-error:0.101686+0.00422301\ttest-error:0.166292+0.0179776\n",
      "[89]\ttrain-error:0.101966+0.00383099\ttest-error:0.167415+0.0179072\n",
      "[90]\ttrain-error:0.101686+0.0036191\ttest-error:0.169663+0.0185989\n",
      "[91]\ttrain-error:0.101966+0.0036191\ttest-error:0.170786+0.0193311\n",
      "[92]\ttrain-error:0.101405+0.00370531\ttest-error:0.169663+0.0185989\n",
      "[93]\ttrain-error:0.101685+0.00431545\ttest-error:0.170786+0.0168914\n",
      "[94]\ttrain-error:0.100281+0.00403158\ttest-error:0.169663+0.0185989\n",
      "[95]\ttrain-error:0.101124+0.00444155\ttest-error:0.169663+0.0185989\n",
      "[96]\ttrain-error:0.101124+0.00444155\ttest-error:0.169663+0.0164369\n",
      "[97]\ttrain-error:0.101405+0.00481665\ttest-error:0.170786+0.0149062\n",
      "[98]\ttrain-error:0.101124+0.00494593\ttest-error:0.170786+0.0149062\n",
      "[99]\ttrain-error:0.100843+0.00481656\ttest-error:0.170786+0.0149062\n"
     ]
    }
   ],
   "source": [
    "xbg_param = {\n",
    "    'learning_rate' : 0.025,\n",
    "    'n_estimators' : 1000,\n",
    "    'max_depth' : 5,\n",
    "    'gamma' : 0,\n",
    "    'subsample' : 0.8,\n",
    "    'colsample_bytree' : 0.8,\n",
    "    'objective' : 'binary:logistic',\n",
    "    'nthread' : 4,\n",
    "    'seed' : 27\n",
    "}\n",
    "\n",
    "xgb1 = xgb.XGBClassifier( **xbg_param )\n",
    "\n",
    "xgtrain = xgb.DMatrix(np.array(train), label = np.array(outcomes))\n",
    "cvresult = xgb.cv(xbg_param, xgtrain, num_boost_round = xgb1.get_params()['n_estimators'],\n",
    "                 nfold = 5, metrics = 'error', early_stopping_rounds = 50, verbose_eval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'reg_alpha': 1e-05, 'max_depth': 3, 'gamma': 0.1, 'min_child_weight': 5}\n",
      "Model accuracy (hold-out): 0.826815642458\n",
      "Model accuracy (5-fold): 0.82828876791 \n",
      "\n",
      "CPU times: user 34 s, sys: 648 ms, total: 34.7 s\n",
      "Wall time: 36min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm_model = train_test_model(\n",
    "    xgb.XGBClassifier(learning_rate = 0.025, n_estimators = 99), \n",
    "    {'max_depth':range(3, 10, 2), 'min_child_weight':range(1, 6, 2),\n",
    "    'gamma': [i / 10.0 for i in range(0, 5)], 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}, \n",
    "    np.array(X_train), np.array(X_test), y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission('lr_model.csv', lr_model, train, outcomes, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission('rf_model.csv', rf_model, train, outcomes, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission('svm_model.csv', svm_model, train, outcomes, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission('gbm_model.csv', gbm_model, train, outcomes, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
