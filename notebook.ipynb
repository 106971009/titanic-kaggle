{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is going to be to combine the training and test sets so that any data transformations / feature engineering is easily applied to both. Only the training set is labeled, so I will create values of -999 for `Survived` in the test subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.concat(\n",
    "    [train.assign(Train = 1), \n",
    "    test.assign(Train = 0).assign(Survived = -999)[list(train) + ['Train']]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with `Name` - create last name, title, family features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_lastname = lambda x: x.split(',')[0]\n",
    "\n",
    "def extract_title(x):\n",
    "    title = x.split(',')[1].split('.')[0][1:]\n",
    "    if title in ['Mlle', 'Ms']:\n",
    "        title = 'Miss'\n",
    "    elif title == 'Mme':\n",
    "        title = 'Mrs'\n",
    "    elif title in ['Rev', 'Dr', 'Major', 'Col', 'Capt', 'Jonkheer', 'Dona']:\n",
    "        title = 'Esteemed'\n",
    "    elif title in ['Don', 'Lady', 'Sir', 'the Countess']:\n",
    "        title = 'Royalty'\n",
    "    return title\n",
    "    \n",
    "data = (data\n",
    "    .assign(LastName = lambda x: x.Name.map(extract_lastname))\n",
    "    .assign(Title = lambda x: x.Name.map(extract_title))\n",
    "    .assign(FamSize = lambda x: x.SibSp + x.Parch + 1)\n",
    "    .assign(Family = lambda x: [a + '_' + str(b) for a, b in \n",
    "                                zip(list(x.LastName), list(x.FamSize))])\n",
    "    .drop(['Name', 'SibSp', 'Parch', 'LastName'], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with `Ticket` - to reduce overfitting, create dummy variables for tickets that are shared among two or more passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ticket_counts(data):\n",
    "    ticket_to_count = dict(data.Ticket.value_counts())\n",
    "    data['TicketCount'] = data['Ticket'].map(ticket_to_count.get)\n",
    "    data['Ticket'] = np.where(data['TicketCount'] > 1, data['Ticket'], np.nan)\n",
    "    return data.drop(['TicketCount'], axis = 1)\n",
    "\n",
    "data = data.pipe(ticket_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the `Cabin` feature - creating a deck feature (the letter in the cabin name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_letter = np.vectorize(lambda x: x[:1]) \n",
    "\n",
    "data = (data\n",
    "        .assign(Deck = lambda x: np.where(\n",
    "            pd.notnull(x.Cabin), first_letter(x.Cabin.fillna('z')), x.Cabin))\n",
    "        .assign(Deck = lambda x: np.where(x.Deck == 'T', np.nan, x.Deck))\n",
    "        .drop(['Cabin'], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns we don't need, convert Sex to a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (data\n",
    "        .drop(['PassengerId'], axis = 1)\n",
    "        .assign(Sex = lambda x: np.where(x.Sex == 'male', 1, 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy variables for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dummy_nans(data, col_name):\n",
    "    deck_cols = [col for col in list(data) if col_name in col]\n",
    "    for deck_col in deck_cols:\n",
    "        data[deck_col] = np.where(\n",
    "            data[col_name + 'nan'] == 1.0, np.nan, data[deck_col])\n",
    "    return data.drop([col_name + 'nan'], axis = 1)\n",
    "\n",
    "data = (data\n",
    "        .assign(Pclass = lambda x: x.Pclass.astype(str))\n",
    "        .pipe(pd.get_dummies, columns = ['Pclass', 'Family', 'Title', 'Ticket'])\n",
    "        .pipe(pd.get_dummies, columns = ['Deck'], dummy_na = True)\n",
    "        .pipe(pd.get_dummies, columns = ['Embarked'], dummy_na = True)\n",
    "        .pipe(create_dummy_nans, 'Deck_')\n",
    "        .pipe(create_dummy_nans, 'Embarked_')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise distances between 1309 samples\n",
      "Computing distances for sample #1/1309, elapsed time: 1.810\n",
      "Computing distances for sample #101/1309, elapsed time: 2.458\n",
      "Computing distances for sample #201/1309, elapsed time: 3.096\n",
      "Computing distances for sample #301/1309, elapsed time: 3.738\n",
      "Computing distances for sample #401/1309, elapsed time: 4.380\n",
      "Computing distances for sample #501/1309, elapsed time: 5.019\n",
      "Computing distances for sample #601/1309, elapsed time: 5.694\n",
      "Computing distances for sample #701/1309, elapsed time: 6.336\n",
      "Computing distances for sample #801/1309, elapsed time: 6.988\n",
      "Computing distances for sample #901/1309, elapsed time: 7.629\n",
      "Computing distances for sample #1001/1309, elapsed time: 8.270\n",
      "Computing distances for sample #1101/1309, elapsed time: 8.917\n",
      "Computing distances for sample #1201/1309, elapsed time: 9.576\n",
      "Computing distances for sample #1301/1309, elapsed time: 10.229\n",
      "Imputing row 1/1309 with 7 missing columns, elapsed time: 10.394\n",
      "Imputing row 101/1309 with 7 missing columns, elapsed time: 10.408\n",
      "Imputing row 201/1309 with 7 missing columns, elapsed time: 10.420\n",
      "Imputing row 301/1309 with 8 missing columns, elapsed time: 10.432\n",
      "Imputing row 401/1309 with 7 missing columns, elapsed time: 10.443\n",
      "Imputing row 501/1309 with 7 missing columns, elapsed time: 10.456\n",
      "Imputing row 601/1309 with 7 missing columns, elapsed time: 10.469\n",
      "Imputing row 701/1309 with 0 missing columns, elapsed time: 10.482\n",
      "Imputing row 801/1309 with 7 missing columns, elapsed time: 10.493\n",
      "Imputing row 901/1309 with 7 missing columns, elapsed time: 10.505\n",
      "Imputing row 1001/1309 with 0 missing columns, elapsed time: 10.517\n",
      "Imputing row 1101/1309 with 7 missing columns, elapsed time: 10.529\n",
      "Imputing row 1201/1309 with 7 missing columns, elapsed time: 10.542\n",
      "Imputing row 1301/1309 with 7 missing columns, elapsed time: 10.553\n",
      "[KNN] Warning: 4/1527603 still missing after imputation, replacing with 0\n",
      "Number of NAs: 0\n"
     ]
    }
   ],
   "source": [
    "def impute(data):\n",
    "    impute_missing = data.drop(['Survived', 'Train'], axis = 1)\n",
    "    impute_missing_cols = list(impute_missing)\n",
    "    filled_soft = fancyimpute.KNN().complete(np.array(impute_missing))\n",
    "    results = pd.DataFrame(filled_soft, columns = impute_missing_cols)\n",
    "    results['Train'] = list(data['Train'])\n",
    "    results['Survived'] = list(data['Survived'])\n",
    "    assert results.isnull().sum().sum() == 0, 'Not all NAs removed'\n",
    "    return results\n",
    "\n",
    "data = data.pipe(impute)\n",
    "print 'Number of NAs:', data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into separate training and predicting sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcomes = np.array(data.query('Train == 1')['Survived'])\n",
    "train = (data.query('Train == 1')\n",
    "         .drop(['Train', 'Survived'], axis = 1))\n",
    "to_predict = (data.query('Train == 0')\n",
    "              .drop(['Train', 'Survived'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, randomly split the training set into training and test sets using hold-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train, outcomes, test_size = 0.2, random_state = 50)\n",
    "\n",
    "def create_submission(name, model, train, outcomes, to_predict):\n",
    "    \"\"\"\n",
    "    Train [model] on [train] and predict the probabilties on [test], and\n",
    "    format the submission according to Kaggle.\n",
    "    \"\"\"\n",
    "    model.fit(np.array(train), outcomes)\n",
    "    probs = model.predict(np.array(to_predict))\n",
    "    results = pd.DataFrame(probs, columns = ['Survived'])\n",
    "    results['PassengerId'] = list(pd.read_csv('data/test.csv')['PassengerId'])\n",
    "    (results[['PassengerId', 'Survived']]\n",
    "        .to_csv('submissions/' + name, index = False))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use grid search to optimize hyperparameters for a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def train_test_model(model, hyperparameters, X_train, X_test, y_train, y_test,\n",
    "                    folds = 5):\n",
    "    \"\"\"\n",
    "    Given a [model] and a set of possible [hyperparameters], along with \n",
    "    matricies corresponding to hold-out cross-validation, returns a model w/ \n",
    "    optimized hyperparameters, and prints out model evaluation metrics.\n",
    "    \"\"\"\n",
    "    optimized_model = GridSearchCV(model, hyperparameters, cv = folds, n_jobs = -1)\n",
    "    optimized_model.fit(X_train, y_train)\n",
    "    predicted = optimized_model.predict(X_test)\n",
    "    print 'Optimized parameters:', optimized_model.best_params_\n",
    "    print 'Model accuracy:', optimized_model.score(X_test, y_test), '\\n'\n",
    "    return optimized_model\n",
    "\n",
    "param_grid = {'n_estimators': [10, 50, 100, 300, 500, 800, 1000, 2000],\n",
    "             'max_depth': [3, 7, None],\n",
    "             'max_features': ['auto', 'log2', None],\n",
    "             'min_samples_leaf': [1, 3, 10],\n",
    "             'min_samples_split': [1, 2, 10]}\n",
    "rf_model = train_test_model(\n",
    "    RandomForestClassifier(), param_grid, X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "np.mean(cross_val_score(rf_model, np.array(train), outcomes, cv = 10))\n",
    "create_submission('rf_model5.csv', rf_model, train, outcomes, to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
